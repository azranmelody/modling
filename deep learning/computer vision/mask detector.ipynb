{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mtcnn import MTCNN\n",
    "import cv2\n",
    "detector = MTCNN()\n",
    "#Load a videopip TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000027D4C377790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "video_capture = cv2.VideoCapture(0)\n",
    " \n",
    "while (True):\n",
    "    ret, frame = video_capture.read()\n",
    "    frame = cv2.resize(frame, (600, 400))\n",
    "    boxes = detector.detect_faces(frame)\n",
    "    if boxes:\n",
    " \n",
    "        box = boxes[0]['box']\n",
    "        conf = boxes[0]['confidence']\n",
    "        x, y, w, h = box[0], box[1], box[2], box[3]\n",
    " \n",
    "        if conf > 0.5:\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 255, 255), 3)\n",
    " \n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    if cv2.waitKey(1) == 13: #13 is the Enter Key\n",
    "        break\n",
    " \n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model,load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Melody\\\\Documents\\\\workspace\\\\computer vision'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagePaths = list(paths.list_images('dataset-20201016T175141Z-001/dataset'))\n",
    "data = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dataset-20201016T175141Z-001/dataset', 'without_mask', '0.jpg']"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imagePaths[0].split(os.path.sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\PIL\\Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "for imagePath in imagePaths:\n",
    "\tlabel = imagePath.split(os.path.sep)[-2]\n",
    "\t# load the input image (224x224) and preprocess it\n",
    "\timage = load_img(imagePath, target_size=(224, 224))\n",
    "\timage = img_to_array(image)\n",
    "\timage = preprocess_input(image)\n",
    "\t# update the data and labels lists, respectively\n",
    "\tdata.append(image)\n",
    "\tlabels.append(label)\n",
    "# convert the data and labels to NumPy arrays\n",
    "data = np.array(data, dtype=\"float32\")\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3957"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3957,)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    cv2.imshow(\"Tobago\", data[-100+i]) \n",
    "\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseModel = MobileNetV2(weights=\"imagenet\", include_top=False,\n",
    "\tinput_shape=(224, 224, 3))\n",
    "# construct the head of the model that will be placed on top of the\n",
    "# the base model\n",
    "headModel = baseModel.output\n",
    "headModel = AveragePooling2D(pool_size=(7, 7))(headModel)\n",
    "headModel = Flatten(name=\"flatten\")(headModel)\n",
    "headModel = Dense(128, activation=\"relu\")(headModel)\n",
    "headModel = Dropout(0.3)(headModel)\n",
    "headModel = Dense(2, activation=\"softmax\")(headModel)\n",
    "\n",
    "# place the head FC model on top of the base model (this will become\n",
    "# the actual model we will train)\n",
    "model = Model(inputs=baseModel.input, outputs=headModel)\n",
    "# loop over all layers in the base model and freeze them so they will\n",
    "# *not* be updated during the first training process\n",
    "for layer in baseModel.layers:\n",
    "\tlayer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = LabelBinarizer()\n",
    "labels = lb.fit_transform(labels)\n",
    "labels = to_categorical(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data for training and the remaining 20% for testing\n",
    "(trainX, testX, trainY, testY) = train_test_split(data, labels,\n",
    "\ttest_size=0.20, stratify=labels, random_state=42)\n",
    "#construct the training image generator for data augmentation\n",
    "aug = ImageDataGenerator(\n",
    "\trotation_range=40,\n",
    "\tzoom_range=0.3,\n",
    "\twidth_shift_range=0.2,\n",
    "\theight_shift_range=0.2,\n",
    "\tshear_range=0.15,\n",
    "\thorizontal_flip=True,\n",
    "\tfill_mode=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "##calssbacks:\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath='saved_model/',\n",
    "                                                        save_best_only=True,\n",
    "                                                        monitor='accuracy',\n",
    "                                                        save_weights_only=True,\n",
    "                                                        mode='max',\n",
    "                                                        save_freq='epoch')\n",
    "\n",
    "earlyStopping_callback = tf.keras.callbacks.EarlyStopping(monitor='accuracy',\n",
    "                                                         patience=15,\n",
    "                                                         mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3165, 224, 224, 3)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n",
      "[INFO] training head...\n",
      "Epoch 1/50\n",
      "98/98 [==============================] - 129s 1s/step - loss: 0.4081 - accuracy: 0.8174 - val_loss: 0.1812 - val_accuracy: 0.9520\n",
      "Epoch 2/50\n",
      "98/98 [==============================] - 118s 1s/step - loss: 0.1880 - accuracy: 0.9365 - val_loss: 0.1365 - val_accuracy: 0.9558\n",
      "Epoch 3/50\n",
      "98/98 [==============================] - 120s 1s/step - loss: 0.1637 - accuracy: 0.9403 - val_loss: 0.1201 - val_accuracy: 0.9634\n",
      "Epoch 4/50\n",
      "98/98 [==============================] - 118s 1s/step - loss: 0.1278 - accuracy: 0.9582 - val_loss: 0.1156 - val_accuracy: 0.9659\n",
      "Epoch 5/50\n",
      "98/98 [==============================] - 124s 1s/step - loss: 0.1223 - accuracy: 0.9566 - val_loss: 0.1031 - val_accuracy: 0.9659\n",
      "Epoch 6/50\n",
      "98/98 [==============================] - 122s 1s/step - loss: 0.1168 - accuracy: 0.9620 - val_loss: 0.0978 - val_accuracy: 0.9659\n",
      "Epoch 7/50\n",
      "98/98 [==============================] - 122s 1s/step - loss: 0.1043 - accuracy: 0.9633 - val_loss: 0.0986 - val_accuracy: 0.9659\n",
      "Epoch 8/50\n",
      "98/98 [==============================] - 128s 1s/step - loss: 0.1081 - accuracy: 0.9662 - val_loss: 0.0960 - val_accuracy: 0.9659\n",
      "Epoch 9/50\n",
      "98/98 [==============================] - 119s 1s/step - loss: 0.1062 - accuracy: 0.9633 - val_loss: 0.0948 - val_accuracy: 0.9684\n",
      "Epoch 10/50\n",
      "98/98 [==============================] - 119s 1s/step - loss: 0.0963 - accuracy: 0.9658 - val_loss: 0.0880 - val_accuracy: 0.9684\n",
      "Epoch 11/50\n",
      "98/98 [==============================] - 137s 1s/step - loss: 0.0951 - accuracy: 0.9671 - val_loss: 0.0957 - val_accuracy: 0.9646\n",
      "Epoch 12/50\n",
      "98/98 [==============================] - 143s 1s/step - loss: 0.0955 - accuracy: 0.9697 - val_loss: 0.0890 - val_accuracy: 0.9697\n",
      "Epoch 13/50\n",
      "98/98 [==============================] - 129s 1s/step - loss: 0.0901 - accuracy: 0.9694 - val_loss: 0.0955 - val_accuracy: 0.9672\n",
      "Epoch 14/50\n",
      "98/98 [==============================] - 136s 1s/step - loss: 0.0937 - accuracy: 0.9719 - val_loss: 0.0900 - val_accuracy: 0.9684\n",
      "Epoch 15/50\n",
      "98/98 [==============================] - 127s 1s/step - loss: 0.0850 - accuracy: 0.9722 - val_loss: 0.0836 - val_accuracy: 0.9672\n",
      "Epoch 16/50\n",
      "98/98 [==============================] - 126s 1s/step - loss: 0.0872 - accuracy: 0.9681 - val_loss: 0.0823 - val_accuracy: 0.9684\n",
      "Epoch 17/50\n",
      "98/98 [==============================] - 128s 1s/step - loss: 0.0809 - accuracy: 0.9745 - val_loss: 0.0839 - val_accuracy: 0.9697\n",
      "Epoch 18/50\n",
      "98/98 [==============================] - 139s 1s/step - loss: 0.0804 - accuracy: 0.9738 - val_loss: 0.0795 - val_accuracy: 0.9672\n",
      "Epoch 19/50\n",
      "98/98 [==============================] - 138s 1s/step - loss: 0.0772 - accuracy: 0.9716 - val_loss: 0.0804 - val_accuracy: 0.9684\n",
      "Epoch 20/50\n",
      "98/98 [==============================] - 141s 1s/step - loss: 0.0807 - accuracy: 0.9710 - val_loss: 0.0759 - val_accuracy: 0.9684\n",
      "Epoch 21/50\n",
      "98/98 [==============================] - 138s 1s/step - loss: 0.0702 - accuracy: 0.9732 - val_loss: 0.0763 - val_accuracy: 0.9697\n",
      "Epoch 22/50\n",
      "98/98 [==============================] - 139s 1s/step - loss: 0.0721 - accuracy: 0.9748 - val_loss: 0.0802 - val_accuracy: 0.9722\n",
      "Epoch 23/50\n",
      "98/98 [==============================] - 141s 1s/step - loss: 0.0718 - accuracy: 0.9761 - val_loss: 0.0831 - val_accuracy: 0.9735\n",
      "Epoch 24/50\n",
      "98/98 [==============================] - 143s 1s/step - loss: 0.0745 - accuracy: 0.9729 - val_loss: 0.0822 - val_accuracy: 0.9710\n",
      "Epoch 25/50\n",
      "98/98 [==============================] - 140s 1s/step - loss: 0.0697 - accuracy: 0.9773 - val_loss: 0.0787 - val_accuracy: 0.9710\n",
      "Epoch 26/50\n",
      "98/98 [==============================] - 131s 1s/step - loss: 0.0732 - accuracy: 0.9770 - val_loss: 0.0892 - val_accuracy: 0.9697\n",
      "Epoch 27/50\n",
      "98/98 [==============================] - 137s 1s/step - loss: 0.0753 - accuracy: 0.9764 - val_loss: 0.0763 - val_accuracy: 0.9722\n",
      "Epoch 28/50\n",
      "98/98 [==============================] - 151s 2s/step - loss: 0.0674 - accuracy: 0.9786 - val_loss: 0.0755 - val_accuracy: 0.9672\n",
      "Epoch 29/50\n",
      "98/98 [==============================] - 145s 1s/step - loss: 0.0671 - accuracy: 0.9754 - val_loss: 0.0763 - val_accuracy: 0.9722\n",
      "Epoch 30/50\n",
      "98/98 [==============================] - 144s 1s/step - loss: 0.0635 - accuracy: 0.9793 - val_loss: 0.0751 - val_accuracy: 0.9684\n",
      "Epoch 31/50\n",
      "98/98 [==============================] - 132s 1s/step - loss: 0.0642 - accuracy: 0.9764 - val_loss: 0.0746 - val_accuracy: 0.9710\n",
      "Epoch 32/50\n",
      "98/98 [==============================] - 136s 1s/step - loss: 0.0664 - accuracy: 0.9767 - val_loss: 0.0745 - val_accuracy: 0.9710\n",
      "Epoch 33/50\n",
      "98/98 [==============================] - 134s 1s/step - loss: 0.0625 - accuracy: 0.9802 - val_loss: 0.0756 - val_accuracy: 0.9710\n",
      "Epoch 34/50\n",
      "98/98 [==============================] - 145s 1s/step - loss: 0.0621 - accuracy: 0.9796 - val_loss: 0.0859 - val_accuracy: 0.9710\n",
      "Epoch 35/50\n",
      "98/98 [==============================] - 134s 1s/step - loss: 0.0609 - accuracy: 0.9780 - val_loss: 0.0811 - val_accuracy: 0.9710\n",
      "Epoch 36/50\n",
      "98/98 [==============================] - 134s 1s/step - loss: 0.0604 - accuracy: 0.9780 - val_loss: 0.0874 - val_accuracy: 0.9697\n",
      "Epoch 37/50\n",
      "98/98 [==============================] - 135s 1s/step - loss: 0.0590 - accuracy: 0.9786 - val_loss: 0.0726 - val_accuracy: 0.9710\n",
      "Epoch 38/50\n",
      "98/98 [==============================] - 133s 1s/step - loss: 0.0607 - accuracy: 0.9786 - val_loss: 0.0689 - val_accuracy: 0.9710\n",
      "Epoch 39/50\n",
      "98/98 [==============================] - 131s 1s/step - loss: 0.0587 - accuracy: 0.9802 - val_loss: 0.0690 - val_accuracy: 0.9722\n",
      "Epoch 40/50\n",
      "98/98 [==============================] - 126s 1s/step - loss: 0.0550 - accuracy: 0.9824 - val_loss: 0.0663 - val_accuracy: 0.9722\n",
      "Epoch 41/50\n",
      "98/98 [==============================] - 128s 1s/step - loss: 0.0523 - accuracy: 0.9831 - val_loss: 0.0791 - val_accuracy: 0.9722\n",
      "Epoch 42/50\n",
      "98/98 [==============================] - 125s 1s/step - loss: 0.0613 - accuracy: 0.9789 - val_loss: 0.0720 - val_accuracy: 0.9710\n",
      "Epoch 43/50\n",
      "98/98 [==============================] - 126s 1s/step - loss: 0.0594 - accuracy: 0.9815 - val_loss: 0.0790 - val_accuracy: 0.9710\n",
      "Epoch 44/50\n",
      "98/98 [==============================] - 126s 1s/step - loss: 0.0539 - accuracy: 0.9834 - val_loss: 0.0732 - val_accuracy: 0.9722\n",
      "Epoch 45/50\n",
      "98/98 [==============================] - 126s 1s/step - loss: 0.0573 - accuracy: 0.9764 - val_loss: 0.0760 - val_accuracy: 0.9722\n",
      "Epoch 46/50\n",
      "98/98 [==============================] - 127s 1s/step - loss: 0.0521 - accuracy: 0.9828 - val_loss: 0.0778 - val_accuracy: 0.9747\n",
      "Epoch 47/50\n",
      "98/98 [==============================] - 126s 1s/step - loss: 0.0513 - accuracy: 0.9818 - val_loss: 0.0692 - val_accuracy: 0.9735\n",
      "Epoch 48/50\n",
      "98/98 [==============================] - 126s 1s/step - loss: 0.0508 - accuracy: 0.9850 - val_loss: 0.0780 - val_accuracy: 0.9710\n",
      "Epoch 49/50\n",
      "98/98 [==============================] - 131s 1s/step - loss: 0.0575 - accuracy: 0.9777 - val_loss: 0.0692 - val_accuracy: 0.9747\n",
      "Epoch 50/50\n",
      "98/98 [==============================] - 139s 1s/step - loss: 0.0456 - accuracy: 0.9850 - val_loss: 0.0722 - val_accuracy: 0.9747\n"
     ]
    }
   ],
   "source": [
    "INIT_LR = 1e-4\n",
    "EPOCHS = 50\n",
    "BS = 32\n",
    "print(\"[INFO] compiling model...\")\n",
    "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt,\n",
    "\tmetrics=[\"accuracy\"])\n",
    "# train the head of the network\n",
    "print(\"[INFO] training head...\")\n",
    "H = model.fit(\n",
    "\taug.flow(trainX, trainY, batch_size=BS),\n",
    "\tsteps_per_epoch=len(trainX) // BS,\n",
    "\tvalidation_data=(testX, testY),\n",
    "\tvalidation_steps=len(testX) // BS,\n",
    "\tepochs=EPOCHS,\n",
    "    callbacks=[checkpoint_callback,earlyStopping_callback]            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = load_model(\"saved_model/mask_recog_ver2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(480, 640)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gray.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "import numpy as np\n",
    " \n",
    "cascPath = os.path.dirname(\n",
    "    cv2.__file__) + \"/data/haarcascade_frontalface_alt2.xml\"\n",
    "faceCascade = cv2.CascadeClassifier(cascPath)\n",
    "#model = load_model(\"saved_model/mask_recog_ver2.h5\")\n",
    " \n",
    "video_capture = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = video_capture.read()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = faceCascade.detectMultiScale(gray,\n",
    "                                         scaleFactor=1.1,\n",
    "                                         minNeighbors=5,\n",
    "                                         minSize=(60, 60),\n",
    "                                         flags = cv2.CASCADE_SCALE_IMAGE|cv2.CASCADE_FIND_BIGGEST_OBJECT|cv2.CASCADE_DO_ROUGH_SEARCH)\n",
    "    faces_list=[]\n",
    "    preds=[]\n",
    "    for (x, y, w, h) in faces:\n",
    "        face_frame = frame[y:y+h,x:x+w]\n",
    "        face_frame = cv2.cvtColor(face_frame, cv2.COLOR_BGR2RGB)\n",
    "        face_frame = cv2.resize(face_frame, (224, 224))\n",
    "        face_frame = img_to_array(face_frame)\n",
    "        face_frame = np.expand_dims(face_frame, axis=0)\n",
    "        face_frame =  preprocess_input(face_frame)\n",
    "        faces_list.append(face_frame)\n",
    "        if len(faces_list)>0:\n",
    "            preds = model.predict(faces_list)\n",
    "        for pred in preds:\n",
    "            (mask, withoutMask) = pred\n",
    "        #label = \"Mask\" if mask > withoutMask else \"No Mask\"\n",
    "        label = \"Is Mask ->\" + str(mask)\n",
    "        #color = (0, 255, 0) if label == \"Mask\" else (0, 0, 255)\n",
    "        color = (0, 255, 0) if mask>0.985  else (0, 0, 255)\n",
    "        #color = (255,255,255)\n",
    "        #label = \"{}: {:.2f}%\".format(label, max(mask, withoutMask) * 100)\n",
    "        cv2.putText(frame, label, (x, y- 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n",
    " \n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h),color, 2)\n",
    "        # Display the resulting frame\n",
    "    cv2.imshow('Video', frame)\n",
    "    if cv2.waitKey(1) ==13:\n",
    "        break\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save(filepath='saved_model/improvement_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('saved_model/improvement_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "import numpy as np\n",
    " \n",
    "cascPath = os.path.dirname(\n",
    "    cv2.__file__) + \"/data/haarcascade_frontalface_alt2.xml\"\n",
    "faceCascade = cv2.CascadeClassifier('haarcascads/frontalEyes35x16.xml')\n",
    "profileFace = cv2.CascadeClassifier('haarcascads/haarcascade_eye.xml')\n",
    "#model = load_model(\"saved_model/mask_recog_ver2.h5\")\n",
    "\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    \n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = video_capture.read()\n",
    "    \n",
    "    alpha = 1.2 # Contrast control (1.0-3.0)\n",
    "    beta = 20 # Brightness control (0-100)\n",
    "\n",
    "    frame = cv2.convertScaleAbs(frame, alpha=alpha, beta=beta)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    faces = faceCascade.detectMultiScale(gray,\n",
    "                                         scaleFactor=1.1,\n",
    "                                         minNeighbors=5,\n",
    "                                         minSize=(45, 45),\n",
    "                                         flags = cv2.CASCADE_SCALE_IMAGE|cv2.CASCADE_FIND_BIGGEST_OBJECT|cv2.CASCADE_DO_ROUGH_SEARCH)\n",
    "    \n",
    "    h_dup_by = 3\n",
    "    w_dup_by = 1\n",
    "    \n",
    "    if(len(faces)==0):\n",
    "        faces = profileFace.detectMultiScale(gray,\n",
    "                                             scaleFactor=1.1,\n",
    "                                             minNeighbors=5,\n",
    "                                             minSize=(45, 45),\n",
    "                                             flags = cv2.CASCADE_SCALE_IMAGE|cv2.CASCADE_FIND_BIGGEST_OBJECT|cv2.CASCADE_DO_ROUGH_SEARCH)\n",
    "\n",
    "    \n",
    "        h_dup_by = 3\n",
    "        w_dup_by = 3\n",
    "    \n",
    "    faces_list=[]\n",
    "    preds=[]\n",
    "    for (x, y, w, h) in faces:\n",
    "        y=y-50\n",
    "        if (y<=0):\n",
    "            y=0\n",
    "        h=h*h_dup_by\n",
    "        w=w*w_dup_by\n",
    "        face_frame = frame[y:y+h,x:x+w]\n",
    "        face_frame = cv2.cvtColor(face_frame, cv2.COLOR_BGR2RGB)\n",
    "        face_frame = cv2.resize(face_frame, (224, 224))\n",
    "        face_frame = img_to_array(face_frame)\n",
    "        face_frame = np.expand_dims(face_frame, axis=0)\n",
    "        face_frame =  preprocess_input(face_frame)\n",
    "        faces_list.append(face_frame)\n",
    "        if len(faces_list)>0:\n",
    "            preds = model.predict(faces_list)\n",
    "        for pred in preds:\n",
    "            (mask, withoutMask) = pred\n",
    "        label = \"Mask  \" if mask > 0.99 else \"No Mask  \"\n",
    "        label = label + str(round(mask,4))\n",
    "        #label = \"Mask\" + str(mask)\n",
    "        #color = (0, 255, 0) if label == \"Mask\" else (0, 0, 255)\n",
    "        color = (0, 255, 0) if mask>0.99  else (0, 0, 255)\n",
    "        #color = (255,255,255)\n",
    "        #label = \"{}: {:.2f}%\".format(label, max(mask, withoutMask) * 100)\n",
    "        cv2.putText(frame, label, (x, y- 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n",
    " \n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h),color, 2)\n",
    "        # Display the resulting frame\n",
    "    cv2.imshow('Video', frame)\n",
    "    if cv2.waitKey(1) ==13:\n",
    "        break\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### from video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# for _, __, files in os.walk(\".\"):\n",
    "#     for file in files:\n",
    "#         print file\n",
    "# print \"\"\n",
    "\n",
    "# cap = cv2.VideoCapture(\"office video/poc.mp4\")\n",
    "\n",
    "# cap.set(cv2.CAP_PROP_FPS, 1000)\n",
    "# i = 0\n",
    "# while  i < 500:\n",
    "#     i += 1\n",
    "#     #if (i%3==0):\n",
    "#     ret, frame = cap.read()\n",
    "#     frame=cv2.rotate(frame, cv2.ROTATE_90_CLOCKWISE)\n",
    "#     #gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "#     cv2.imshow('frame', frame)\n",
    "#     if cv2.waitKey(15) & 0xFF == ord('q'):\n",
    "#         break\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "import numpy as np\n",
    " \n",
    "cascPath = os.path.dirname(\n",
    "    cv2.__file__) + \"/data/haarcascade_frontalface_alt2.xml\"\n",
    "#faceCascade = cv2.CascadeClassifier('haarcascads/frontalEyes35x16.xml')\n",
    "faceCascade = cv2.CascadeClassifier('haarcascads/haarcascade_upperbody.xml')\n",
    "#profileFace = cv2.CascadeClassifier('haarcascads/haarcascade_eye.xml')\n",
    "#model = load_model(\"saved_model/mask_recog_ver2.h5\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(\"office video/poc.mp4\")\n",
    "\n",
    "cap.set(cv2.CAP_PROP_FPS, 1000)\n",
    "i = 0\n",
    "# while  i < 500:\n",
    "#     i += 1\n",
    "#     #if (i%3==0):\n",
    "#     ret, frame = cap.read()\n",
    "#     frame=cv2.rotate(frame, cv2.ROTATE_90_CLOCKWISE)\n",
    "    #gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "#     cv2.imshow('frame', frame)\n",
    "#     if cv2.waitKey(15) & 0xFF == ord('q'):\n",
    "#         break\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()\n",
    " \n",
    "# video_capture = cv2.VideoCapture(0)\n",
    "while  i < 500:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    frame=cv2.rotate(frame, cv2.ROTATE_90_CLOCKWISE)\n",
    "    \n",
    "    alpha = 1.1 # Contrast control (1.0-3.0)\n",
    "    beta = 10 # Brightness control (0-100)\n",
    "\n",
    "    frame = cv2.convertScaleAbs(frame, alpha=alpha, beta=beta)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    faces = faceCascade.detectMultiScale(gray,\n",
    "                                         scaleFactor=1.1,\n",
    "                                         minNeighbors=5,\n",
    "                                         minSize=(60, 60),\n",
    "                                         flags = cv2.CASCADE_SCALE_IMAGE|cv2.CASCADE_FIND_BIGGEST_OBJECT|cv2.CASCADE_DO_ROUGH_SEARCH)\n",
    "    \n",
    "    h_dup_by = 3\n",
    "    w_dup_by = 1\n",
    "    \n",
    "    if(len(faces)==0):\n",
    "        faces = profileFace.detectMultiScale(gray,\n",
    "                                             scaleFactor=1.1,\n",
    "                                             minNeighbors=5,\n",
    "                                             minSize=(60, 60),\n",
    "                                             flags = cv2.CASCADE_SCALE_IMAGE|cv2.CASCADE_FIND_BIGGEST_OBJECT|cv2.CASCADE_DO_ROUGH_SEARCH)\n",
    "\n",
    "    \n",
    "        h_dup_by = 3\n",
    "        w_dup_by = 3\n",
    "    \n",
    "    faces_list=[]\n",
    "    preds=[]\n",
    "    for (x, y, w, h) in faces:\n",
    "        \n",
    "        h=h*h_dup_by\n",
    "        w=w*w_dup_by\n",
    "        face_frame = frame[y:y+h,x:x+w]\n",
    "        face_frame = cv2.cvtColor(face_frame, cv2.COLOR_BGR2RGB)\n",
    "        face_frame = cv2.resize(face_frame, (224, 224))\n",
    "        face_frame = img_to_array(face_frame)\n",
    "        face_frame = np.expand_dims(face_frame, axis=0)\n",
    "        face_frame =  preprocess_input(face_frame)\n",
    "        faces_list.append(face_frame)\n",
    "        if len(faces_list)>0:\n",
    "            preds = model.predict(faces_list)\n",
    "        for pred in preds:\n",
    "            (mask, withoutMask) = pred\n",
    "        label = \"Mask  \" if mask > 0.995 else \"No Mask  \"\n",
    "        label = label + str(round(mask,4))\n",
    "        #label = \"Mask\" + str(mask)\n",
    "        #color = (0, 255, 0) if label == \"Mask\" else (0, 0, 255)\n",
    "        color = (0, 255, 0) if mask>0.985  else (0, 0, 255)\n",
    "        #color = (255,255,255)\n",
    "        #label = \"{}: {:.2f}%\".format(label, max(mask, withoutMask) * 100)\n",
    "        cv2.putText(frame, label, (x, y- 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n",
    " \n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h),color, 2)\n",
    "        # Display the resulting frame\n",
    "    cv2.imshow('Video', frame)\n",
    "    if cv2.waitKey(1) ==13:\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.6358974e-05, 9.9998367e-01],\n",
       "       [9.6657068e-01, 3.3429299e-02],\n",
       "       [9.9977833e-01, 2.2165888e-04],\n",
       "       ...,\n",
       "       [9.9960214e-01, 3.9787785e-04],\n",
       "       [1.8457719e-03, 9.9815422e-01],\n",
       "       [9.9991965e-01, 8.0319129e-05]], dtype=float32)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(trainX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    " \n",
    "# initialize the HOG descriptor/person detector\n",
    "hog = cv2.HOGDescriptor()\n",
    "hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "\n",
    "cv2.startWindowThread()\n",
    "\n",
    "# open webcam video stream\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# the output will be written to output.avi\n",
    "out = cv2.VideoWriter(\n",
    "    'output.avi',\n",
    "    cv2.VideoWriter_fourcc(*'MJPG'),\n",
    "    15.,\n",
    "    (640,480))\n",
    "\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # resizing for faster detection\n",
    "    frame = cv2.resize(frame, (640, 480))\n",
    "    # using a greyscale picture, also for faster detection\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    # detect people in the image\n",
    "    # returns the bounding boxes for the detected objects\n",
    "    boxes, weights = hog.detectMultiScale(frame, winStride=(8,8) )\n",
    "\n",
    "    boxes = np.array([[x, y, x + w, y + h] for (x, y, w, h) in boxes])\n",
    "\n",
    "    for (xA, yA, xB, yB) in boxes:\n",
    "        # display the detected boxes in the colour picture\n",
    "        cv2.rectangle(frame, (xA, yA), (xB, yB),\n",
    "                          (0, 255, 0), 2)\n",
    "    \n",
    "    # Write the output video \n",
    "    out.write(frame.astype('uint8'))\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('frame',frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "# and release the output\n",
    "out.release()\n",
    "# finally, close the window\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_environment",
   "language": "python",
   "name": "new_environment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
