{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZoKh0HAYq-ZS"
   },
   "source": [
    "# Basics of Neural networks and other stuff\n",
    "```In this exercise you will experience working with keras, a useful tool for designing and training neural networks.```\n",
    "\n",
    "```~Ittai Haran```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6lomzfgRq-ZT"
   },
   "source": [
    "## Stage 1- predicting engineered functions\n",
    "```Here you will design a simple fully connected network to predict a few simple functions. You will try different activation functions and different architectures (number of layers, size of layers).```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q9EieSkpq-ZV"
   },
   "source": [
    "```The first experiment will be guided:```\n",
    "- ```Read the first dataframe - function_1.csv.``` (in: https://drive.google.com/open?id=19Y9f2pkUwP7nrgbKCmCGHEomHmA0pwqx)\n",
    "- ```Plot y against x. can you guess the function y(x)?```\n",
    "- ```Split you data to train segment (70%) and test segment (30%).```\n",
    "- ```Write a fully connected neural network with one hidden layer with 3 units. I suggest using the``` [functional API](https://keras.io/getting-started/functional-api-guide/) - ```you can also find there examples for simple working with keras.```\n",
    "- ```Use tanh as an activation function for the hidden layer and a linear activation for the output layer.```\n",
    "- ```Use model.summary() to look at your model's architecture.```\n",
    "- ```Use mean squared error as the loss function and SGD (stochastic gradient descent) as the optimizer.```\n",
    "- ```Try training the network with different batch's sizes (don't be afraid to use many epochs- you don't have a lot of data).```\n",
    "- ```Plot y against x and f(x) against x on the same graph.```\n",
    "- ```Compute the loss on the test segment.```\n",
    "- ```Can you use a smaller hidden layer?```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PURF2UDGq-ZW"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.layers import Dense, Input\n",
    "from keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "omport seaborn sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WmJu_8QBq-Zb"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('function_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WmJu_8QBq-Zb"
   },
   "outputs": [],
   "source": [
    "df.plot.scatter('x','y') # i guess the function is something like |x^2| "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WmJu_8QBq-Zb"
   },
   "outputs": [],
   "source": [
    "Y = df['y']\n",
    "X = df['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WmJu_8QBq-Zb"
   },
   "outputs": [],
   "source": [
    "train_X, test_X, train_Y, test_Y = train_test_split(X, Y, train_size =0.7 , test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WmJu_8QBq-Zb"
   },
   "outputs": [],
   "source": [
    "from keras import Model\n",
    "from keras.layers import Dense, Input\n",
    "from keras.optimizers import SGD\n",
    "from keras.losses import binary_crossentropy\n",
    "\n",
    "inp = Input((1,)) #we got only one feature\n",
    "\n",
    "hidden = Dense(3, activation='tanh')(inp) #one hidden layer which contains 3 nuirons with tanh AF\n",
    "out = Dense(1, activation='linear')(hidden)\n",
    "\n",
    "model = Model(inp, out)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WmJu_8QBq-Zb"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=SGD(), loss='mean_squared_error', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WmJu_8QBq-Zb"
   },
   "outputs": [],
   "source": [
    "# scaler = StandardScaler()\n",
    "# new_train_X = scaler.fit_transform(train_X)\n",
    "# new_test_X = scaler.transform(test_X) \n",
    "# THE DATA IS ALREADY NORMALIZED #\n",
    "\n",
    "model.fit(train_X, train_Y, batch_size=5, epochs=30, validation_data=(test_X, test_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WmJu_8QBq-Zb"
   },
   "outputs": [],
   "source": [
    "test_preds = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WmJu_8QBq-Zb"
   },
   "outputs": [],
   "source": [
    "df_helper = pd.DataFrame(test_X).reset_index(drop=True)\n",
    "df_helper = df_helper.join(pd.DataFrame(test_preds , columns=['y_predicted_is_f(x)']).reset_index(drop=True)).join(pd.DataFrame(test_Y).reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WmJu_8QBq-Zb"
   },
   "outputs": [],
   "source": [
    "#!pip install seaborn --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WmJu_8QBq-Zb"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WmJu_8QBq-Zb"
   },
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "sns.scatterplot(x='x', y='y',\n",
    "                     data=df_helper)\n",
    "\n",
    "sns.scatterplot(x='x', y='y_predicted_is_f(x)',\n",
    "                     data=df_helper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WmJu_8QBq-Zb"
   },
   "outputs": [],
   "source": [
    "mean_squared_error(test_Y , test_preds) #the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WmJu_8QBq-Zb"
   },
   "outputs": [],
   "source": [
    "#trying useing smaller hidden layer: containing only one nuiron.\n",
    "from keras import Model\n",
    "from keras.layers import Dense, Input\n",
    "from keras.optimizers import SGD\n",
    "from keras.losses import binary_crossentropy\n",
    "\n",
    "inp = Input((1,)) #we got only one feature\n",
    "\n",
    "hidden = Dense(1, activation='tanh')(inp) #one hidden layer which contains 3 nuirons with tanh AF\n",
    "out = Dense(1, activation='linear')(hidden)\n",
    "\n",
    "model = Model(inp, out)\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=SGD(), loss='mean_squared_error', metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_X, train_Y, batch_size=5, epochs=30, validation_data=(test_X, test_Y))\n",
    "test_preds = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WmJu_8QBq-Zb"
   },
   "outputs": [],
   "source": [
    "df_helper = pd.DataFrame(test_X).reset_index(drop=True)\n",
    "df_helper = df_helper.join(pd.DataFrame(test_preds , columns=['y_predicted_is_f(x)']).reset_index(drop=True)).join(pd.DataFrame(test_Y).reset_index(drop=True))\n",
    "\n",
    "sns.scatterplot(x='x', y='y',\n",
    "                     data=df_helper)\n",
    "\n",
    "sns.scatterplot(x='x', y='y_predicted_is_f(x)',\n",
    "                     data=df_helper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WmJu_8QBq-Zb"
   },
   "outputs": [],
   "source": [
    "mean_squared_error(test_Y , test_preds) #the loss - pretty good for that small nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7MjwVu9zq-Ze"
   },
   "source": [
    "```For the second experiment use the dataframe function_2.csv. Here you might need to have more layers. You might want to consider different``` [optimizers](https://keras.io/optimizers/), ```rather than SGD (for example, Adam).```\n",
    "\n",
    "(```function_2.csv can be found in:``` https://drive.google.com/open?id=1Fk32cP-4DSZ6v175UDVz_OChHE-dFLzq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eMgwVm2mq-Zf"
   },
   "outputs": [],
   "source": [
    "df_exp2 = pd.read_csv('function_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eMgwVm2mq-Zf"
   },
   "outputs": [],
   "source": [
    "#vizualize the data \n",
    "sns.set(rc={'figure.figsize':(7,7)})\n",
    "sns.lineplot(x='x', y='y',data=df_exp2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eMgwVm2mq-Zf"
   },
   "outputs": [],
   "source": [
    "X = df_exp2['x']\n",
    "Y = df_exp2['y']\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(X, Y, train_size =0.7 , test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eMgwVm2mq-Zf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Melody\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-bd88136f4e3b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mean_squared_error'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# fit the keras model on the dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'train_X' is not defined"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(20, input_dim=1, activation='linear'))\n",
    "model.add(Dense(20, activation='tanh'))\n",
    "model.add(Dense(3, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the keras model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "# fit the keras model on the dataset\n",
    "model.fit(train_X, train_Y, epochs=10, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eMgwVm2mq-Zf"
   },
   "outputs": [],
   "source": [
    "test_preds = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eMgwVm2mq-Zf"
   },
   "outputs": [],
   "source": [
    "df_helper = pd.DataFrame(test_X).reset_index(drop=True)\n",
    "df_helper = df_helper.join(pd.DataFrame(test_preds , columns=['y_predicted_is_f(x)']).reset_index(drop=True)).join(pd.DataFrame(test_Y).reset_index(drop=True))\n",
    "\n",
    "sns.scatterplot(x='x', y='y',\n",
    "                     data=df_helper)\n",
    "\n",
    "sns.scatterplot(x='x', y='y_predicted_is_f(x)',\n",
    "                     data=df_helper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eMgwVm2mq-Zf"
   },
   "outputs": [],
   "source": [
    "mean_squared_error(test_Y , test_preds) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Be2DgmtIq-Zj"
   },
   "source": [
    "```For the third experiment use the dataframe function_3.csv. Try different activation functions. How many layers you had to take?```\n",
    "\n",
    "(```function_3.csv can be found in:``` https://drive.google.com/open?id=1-XTfy6Wf0_WhEKJ9AnUGENBZRSNXkwNe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aOpmc2l3q-Zk"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "df_exp3 = pd.read_csv('function_3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aOpmc2l3q-Zk"
   },
   "outputs": [],
   "source": [
    "#vizualize the data \n",
    "sns.set(rc={'figure.figsize':(7,7)})\n",
    "sns.lineplot(x='x', y='y',data=df_exp3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aOpmc2l3q-Zk"
   },
   "outputs": [],
   "source": [
    "X = df_exp3['x']\n",
    "Y = df_exp3['y']\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(X, Y, train_size =0.7 , test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aOpmc2l3q-Zk"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "\n",
    "# define the keras model\n",
    "model = Sequential()\n",
    "#model.add(Dense(200, input_dim=1, activation='linear'))\n",
    "model.add(Dense(30, input_dim=1, activation='linear'))\n",
    "#model.add(Dense(100, activation='relu'))\n",
    "#model.add(Dense(150, activation='linear'))\n",
    "model.add(Dense(25, activation='tanh'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "# compile the keras model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "# fit the keras model on the dataset\n",
    "model.fit(train_X, train_Y, epochs=15, batch_size=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "yUt2PlCPq-Zn"
   },
   "outputs": [],
   "source": [
    "test_preds = model.predict(test_X)\n",
    "\n",
    "df_helper = pd.DataFrame(test_X).reset_index(drop=True)\n",
    "df_helper = df_helper.join(pd.DataFrame(test_preds , columns=['y_predicted_is_f(x)']).reset_index(drop=True)).join(pd.DataFrame(test_Y).reset_index(drop=True))\n",
    "\n",
    "sns.scatterplot(x='x', y='y',\n",
    "                     data=df_helper)\n",
    "\n",
    "sns.scatterplot(x='x', y='y_predicted_is_f(x)',\n",
    "                     data=df_helper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yUt2PlCPq-Zn"
   },
   "source": [
    "```As you maybe could've guessed, the function you fitted is``` $y = sin(x)$. ```Hence one might think it would be easy to fit a network with only one neuron, with activation of the sinus function. Try it: define a neural network with a single input and a single output, with no hidden layers, and use a sinus activation, to approximate function_3. Next we will understand why using sinus as an activation function is not a good idea.```\n",
    "\n",
    "```You can use sin_activation provided below as the sinus activation: instead of writing activation='tanh' or activation='sigmoid', use activation=sin_activation.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s4zronM5q-Zo"
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "def sin_activation(vec):\n",
    "    return K.sin(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TH-Jecp3q-Zs"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "LEARNING_RATE =0.25\n",
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(1, input_dim=1,activation=sin_activation))\n",
    "\n",
    "# compile the keras model\n",
    "model.compile(loss='mean_squared_error', optimizer=Adam(lr=LEARNING_RATE), metrics=['accuracy'])\n",
    "# fit the keras model on the dataset\n",
    "model.fit(train_X, train_Y, epochs=3, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TH-Jecp3q-Zs"
   },
   "outputs": [],
   "source": [
    "test_preds = model.predict(test_X)\n",
    "\n",
    "df_helper = pd.DataFrame(test_X).reset_index(drop=True)\n",
    "df_helper = df_helper.join(pd.DataFrame(test_preds , columns=['y_predicted_is_f(x)']).reset_index(drop=True)).join(pd.DataFrame(test_Y).reset_index(drop=True))\n",
    "\n",
    "sns.scatterplot(x='x', y='y',\n",
    "                     data=df_helper)\n",
    "\n",
    "sns.scatterplot(x='x', y='y_predicted_is_f(x)',\n",
    "                     data=df_helper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q6K91Xmdq-Zv"
   },
   "source": [
    "```Let's explore the sinus activation. For this we will simplify the settings. We will generate 3 samples, with ```\n",
    "$x = [0, 0.5, 1]$ ``` and ``` $y=[sin(0), sin(0.5), sin(1)]$. ```We will keep using the simplest neural network, that has one input and one output, with no hidden layers. Hence the possible functions that the network can give us are functions of the form```\n",
    "<center>$y=sin(a\\cdot x+b)$.<center>\n",
    "\n",
    "```We can use the simple setting to compute and visualize the loss surface. As you recall, the loss is given by```\n",
    "<center>$\\sum_i{(sin(a\\cdot x_i+b)-y_i)^2}$<center>\n",
    "\n",
    "```Create a matrix so that in the (i,j) entry there will be the loss for ``` $a = \\frac{2\\pi}{600}\\cdot i$ and $b= \\frac{2\\pi}{600}\\cdot j$. ```Visualize this matrix using plt.imshow.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xfo5Z5rUq-Zx"
   },
   "outputs": [],
   "source": [
    "x = [0,0.5,1]\n",
    "y = np.sin(x)\n",
    "\n",
    "new_x = list(map(lambda x: x  * (2*np.pi)/600.0 , x))\n",
    "new_y = list(map(lambda y: y  * (2*np.pi)/600.0 , y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xfo5Z5rUq-Zx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xfo5Z5rUq-Zx"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "LEARNING_RATE =0.05\n",
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(1, input_dim=1,activation=sin_activation))\n",
    "\n",
    "# compile the keras model\n",
    "model.compile(loss='mean_squared_error', optimizer=Adam(lr=LEARNING_RATE), metrics=['accuracy'])\n",
    "# fit the keras model on the dataset\n",
    "model.fit(x, y, epochs=30, batch_size=1 , verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xfo5Z5rUq-Zx"
   },
   "outputs": [],
   "source": [
    "history = model.history.history['loss']\n",
    "loss_his_sinus_original = pd.DataFrame(history)\n",
    "sns.lineplot(x= loss_his_sinus_original.index , y=loss_his_sinus_original[0])\n",
    "plt.title('X - num of epoch , Y - loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xfo5Z5rUq-Zx"
   },
   "outputs": [],
   "source": [
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(1, input_dim=1,activation=sin_activation))\n",
    "\n",
    "# compile the keras model\n",
    "model.compile(loss='mean_squared_error', optimizer=Adam(lr=LEARNING_RATE), metrics=['accuracy'])\n",
    "# fit the keras model on the dataset\n",
    "model.fit(new_x, new_y, epochs=30, batch_size=1 , verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xfo5Z5rUq-Zx"
   },
   "outputs": [],
   "source": [
    "history = model.history.history['loss']\n",
    "loss_his_sinus_manipulate = pd.DataFrame(history)\n",
    "sns.lineplot(x= loss_his_sinus_manipulate.index , y=loss_his_sinus_manipulate[0])\n",
    "plt.title('X - num of epoch , Y - loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wHaqddUWq-Z1"
   },
   "source": [
    "```Do the same for the beloved activation, tanh. Based on the images, what makes sinus a poor activation function and tanh a good activation function? answer in ``` $\\underline{a\\ cell\\ below}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8DoYjvKpq-Z3"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "LEARNING_RATE =0.05\n",
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(1, input_dim=1,activation='tanh'))\n",
    "\n",
    "# compile the keras model\n",
    "model.compile(loss='mean_squared_error', optimizer=Adam(lr=LEARNING_RATE), metrics=['accuracy'])\n",
    "# fit the keras model on the dataset\n",
    "model.fit(x, y, epochs=30, batch_size=1 , verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8DoYjvKpq-Z3"
   },
   "outputs": [],
   "source": [
    "history = model.history.history['loss']\n",
    "loss_his_tanh_original = pd.DataFrame(history)\n",
    "sns.lineplot(x= loss_his_tanh_original.index , y=loss_his_tanh_original[0])\n",
    "plt.title('X - num of epoch , Y - loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8DoYjvKpq-Z3"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "LEARNING_RATE =0.05\n",
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(1, input_dim=1,activation='tanh'))\n",
    "\n",
    "# compile the keras model\n",
    "model.compile(loss='mean_squared_error', optimizer=Adam(lr=LEARNING_RATE), metrics=['accuracy'])\n",
    "# fit the keras model on the dataset\n",
    "model.fit(new_x, new_y, epochs=30, batch_size=1 , verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8DoYjvKpq-Z3"
   },
   "outputs": [],
   "source": [
    "history = model.history.history['loss']\n",
    "loss_his_tanh_manipulate = pd.DataFrame(history)\n",
    "sns.lineplot(x= loss_his_tanh_manipulate.index , y=loss_his_tanh_manipulate[0])\n",
    "plt.title('X - num of epoch , Y - loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8DoYjvKpq-Z3"
   },
   "outputs": [],
   "source": [
    "#combine plots\n",
    "sns.lineplot(x= loss_his_tanh_manipulate.index , y=loss_his_tanh_manipulate[0])\n",
    "sns.lineplot(x= loss_his_sinus_manipulate.index , y=loss_his_sinus_manipulate[0])\n",
    "plt.legend(['tanh' , 'sinus'])\n",
    "plt.show()\n",
    "sns.lineplot(x= loss_his_tanh_original.index , y=loss_his_tanh_original[0])\n",
    "sns.lineplot(x= loss_his_sinus_original.index , y=loss_his_sinus_original[0])\n",
    "plt.legend(['tanh' , 'sinus'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8DoYjvKpq-Z3"
   },
   "outputs": [],
   "source": [
    "#the tanh activition is more balanced and less affected by the scale change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eiu2ibbwq-Z6"
   },
   "source": [
    "### Bonus:\n",
    "```When training neural networks using keras we can use callback functions: There are functions that are called automatically during the training and can be of several usages. For example, we can use a callback function to save our model to a file upon the ending of an epoch.\n",
    "Below you will find a custom callback that keeps the weights for a and b in your sinus model. Use it like this:```\n",
    "\n",
    "``` python\n",
    "wh = WeightsHistory()\n",
    "model.fit(X_train, Y_train, batch_size = 100, epochs = 10, verbose=2, callbacks=[wh])\n",
    "```\n",
    "\n",
    "```After you try training your model using the sinus activation you can plot the path your model took on the a-b plane. Add to your loss surface visualization a plot showing the a-b values during training.```\n",
    "\n",
    "```Note: after using plt.imshow you got an image of the size``` $600\\times 600$. ```You will have to scale your a-b path accordingly to see it on the same graph. Try zooming (by changing the scale of the image, enlarging the size of your image or using any other mean) to get a better view of the a-b path. Can you explain why didn't you get a convergence?```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mu-uSUTeq-Z7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mu-uSUTeq-Z7"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "class WeightsHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.a_s = []\n",
    "        self.b_s = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        weights = self.model.get_weights()\n",
    "        self.a_s.append(weights[0][0][0])\n",
    "        self.b_s.append(weights[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dn2KA-Ueq-Z-"
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE =0.05\n",
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(1, input_dim=1,activation=sin_activation))\n",
    "\n",
    "\n",
    "# compile the keras model\n",
    "model.compile(loss='mean_squared_error', optimizer=Adam(lr=LEARNING_RATE), metrics=['accuracy'])\n",
    "# fit the keras model on the dataset\n",
    "\n",
    "wh = WeightsHistory()\n",
    "model.fit(x, y, epochs=30, batch_size=1 , verbose=0 , callbacks=[wh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dn2KA-Ueq-Z-"
   },
   "outputs": [],
   "source": [
    "all_weights = pd.DataFrame(list(zip(wh.b_s , wh.a_s)) , columns=['b_s' , 'a_s'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dn2KA-Ueq-Z-"
   },
   "outputs": [],
   "source": [
    "all_weights['epoch'] = pd.qcut(all_weights.index , 30 , labels=range(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dn2KA-Ueq-Z-"
   },
   "outputs": [],
   "source": [
    "loss_history = pd.DataFrame(model.history.history['loss'] , columns=['loss']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dn2KA-Ueq-Z-"
   },
   "outputs": [],
   "source": [
    "#model.history.history['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dn2KA-Ueq-Z-"
   },
   "outputs": [],
   "source": [
    "all_weights_and_loss = all_weights.merge(loss_history , left_on='epoch' , right_on='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dn2KA-Ueq-Z-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dn2KA-Ueq-Z-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dn2KA-Ueq-Z-"
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(x=all_weights_and_loss['epoch'] , y=all_weights_and_loss['b_s'])\n",
    "sns.scatterplot(x=all_weights_and_loss['epoch'] , y=all_weights_and_loss['a_s'])\n",
    "sns.scatterplot(x=all_weights_and_loss['epoch'] , y=all_weights_and_loss['loss'] , markers='x')\n",
    "plt.legend(['b_s' , 'a_s' , 'loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dn2KA-Ueq-Z-"
   },
   "outputs": [],
   "source": [
    "#the path  of the weights throw training:\n",
    "sns.scatterplot(x=all_weights_and_loss['b_s'] , y=all_weights_and_loss['a_s'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wIPMphb0q-aC"
   },
   "source": [
    "## Stage 2- predict digits from MNIST\n",
    "```Use your knowledge to create a good prediction for the MNIST dataset. Note that this a classification problem, and you will have to use a different loss: for example, the binary cross entropy (log loss). Furthermore, you might want to use softmax to generate predictions. You can use activation='softmax' in the last layer of your network.\n",
    "good luck!```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "znq2DY9eq-aD"
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(X_train_images, Y_train_num), (X_test_images, Y_test_num) = mnist.load_data()\n",
    "plt.imshow(X_train_images[0])\n",
    "plt.show()\n",
    "\n",
    "X_train = X_train_images.reshape(-1,784)\n",
    "X_test = X_test_images.reshape(-1,784)\n",
    "Y_train = pd.get_dummies(Y_train_num)\n",
    "Y_test = pd.get_dummies(Y_test_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ncr7FeEfq-aG"
   },
   "outputs": [],
   "source": [
    "len(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ncr7FeEfq-aG"
   },
   "outputs": [],
   "source": [
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ncr7FeEfq-aG"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.losses import logcosh, binary_crossentropy\n",
    "from keras.optimizers import Adam, Nadam, RMSprop , SGD\n",
    "LEARNING_RATE =0.1\n",
    "# define the keras model\n",
    "model = Sequential()\n",
    "#model.add(Dense(200, input_dim=1, activation='linear'))\n",
    "model.add(Dense(64, input_dim=784, activation='relu'))\n",
    "model.add(Dense(64, input_dim=784, activation='relu'))\n",
    "model.add(Dense(64, input_dim=784, activation='relu'))\n",
    "#model.add(Dense(64, input_dim=784, activation='relu'))\n",
    "model.add(Dense(16, input_dim=784, activation='relu'))\n",
    "# model.add(Dense(350, activation='tanh'))\n",
    "# model.add(Dense(180, activation='relu'))\n",
    "# model.add(Dense(50, input_dim=784, activation='tanh'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "# compile the keras model\n",
    "model.compile(loss=binary_crossentropy, optimizer=SGD(), metrics=['accuracy'])\n",
    "# fit the keras model on the dataset\n",
    "model.fit(X_train, Y_train, epochs=10, batch_size=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "ZZ-Nk2LEq-aJ"
   },
   "outputs": [],
   "source": [
    "preds = model.predict(X_test)\n",
    "test_predictions = model.predict_classes(X_test)\n",
    "Y_test['true_label'] = Y_test.apply(lambda x: x[x>0].index[0] , axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "ZZ-Nk2LEq-aJ"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion = confusion_matrix(Y_test['true_label'],test_predictions)\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "ZZ-Nk2LEq-aJ"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "\n",
    "\n",
    "precision, recall, fscore, support = score(Y_test['true_label'],test_predictions)\n",
    "\n",
    "print('precision: {}'.format(precision))\n",
    "print('recall: {}'.format(recall))\n",
    "print('fscore: {}'.format(fscore))\n",
    "print('support: {}'.format(support))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZZ-Nk2LEq-aJ"
   },
   "source": [
    "## Stage 3 - exploring deep neural networks\n",
    "```In this part we will explore the problems with training deep networks.```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o4SwyYEuq-aL"
   },
   "source": [
    "```We will work with the simplest data possible and will try to approximate the identity function, with one feature.\n",
    "Generate 10,000 samples using np.random.random and have ``` $y=x$. ```Create a neural network with a single hidden layer with 3 neurons in it and sigmoid activation. Approximate the identity function.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MrbRTjLLq-aM"
   },
   "outputs": [],
   "source": [
    "x = np.random.random(10000)\n",
    "y = x.copy()\n",
    "\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(x, y, train_size =0.7 , test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MrbRTjLLq-aM"
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE =0.05\n",
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(3, input_dim=1,activation='sigmoid'))\n",
    "model.add(Dense(1, input_dim=1,activation='linear'))\n",
    "\n",
    "# compile the keras model\n",
    "model.compile(loss='mean_squared_error', optimizer=Adam(lr=LEARNING_RATE), metrics=['accuracy'])\n",
    "# fit the keras model on the dataset\n",
    "model.fit(train_X, train_Y, epochs=10, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MrbRTjLLq-aM"
   },
   "outputs": [],
   "source": [
    "preds = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MrbRTjLLq-aM"
   },
   "outputs": [],
   "source": [
    "mean_squared_error(test_Y , preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3YU1Wtjtq-aR"
   },
   "source": [
    "```Add 50 hidden layers to your network, again, with 3 neurons in each layer and sigmoid activatoin. Try approximating the identity function using this network.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TD7g37ooq-aT"
   },
   "outputs": [],
   "source": [
    "# define the keras model\n",
    "from keras.layers import  RepeatVector\n",
    "model = Sequential()\n",
    "model.add(Dense(3, input_dim=1,activation='sigmoid'))\n",
    "for i in range(50):\n",
    "    model.add(Dense(3 ,activation='sigmoid'))\n",
    "\n",
    "model.add(Dense(1 ,activation='linear'))\n",
    "\n",
    "# compile the keras model\n",
    "model.compile(loss='mean_squared_error', optimizer=SGD(), metrics=['accuracy'])\n",
    "# fit the keras model on the dataset\n",
    "model.fit(train_X, train_Y, epochs=10, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S5ILOzfqq-aY"
   },
   "source": [
    "```Why isn't it working? Explain it in ``` $\\underline{a\\ cell\\ below}$. ```In your explanation regard the process of back propagation and the formula of gradients found in the first layers of the network. Regard also the possible values of the derivative of the sigmoid function.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z6X5Pc0Oq-ac"
   },
   "outputs": [],
   "source": [
    "#If the sigmoid functions gives a HIGH or LOW value(Pretty good confidence),\n",
    "#the derivative of that value is LOW. If you get a value at the steepest slope(0.5),\n",
    "#the derivative of that value is HIGH -> and high value will change the original value and therefore will not be identity to the original value thats why approximating the identity function using this network is not possible.\n",
    "#in the back propagation process there will be no big changes if there are some bad predictions beacuse the use of the derivatives is negligible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YKhOG2JBq-af"
   },
   "source": [
    "```Run the cell below. In weight_grads you will have a list of the gradients computed in every layer. For each layer take the maximum absolute value of the gradients. Plot a graph of the maximum values against the number of layer. Also plot it in logarithmic scale. Use it to justify your answer from before in ``` $\\underline{a\\ cell\\ below}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QqtwBOlCq-ag"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras import backend as K\n",
    "\n",
    "def get_weight_grad(model, inputs, outputs):\n",
    "    \"\"\" Gets gradient of model for given inputs and outputs for all weights\"\"\"\n",
    "    grads = model.optimizer.get_gradients(model.total_loss, model.trainable_weights)\n",
    "    symb_inputs = (model._feed_inputs + model._feed_targets + model._feed_sample_weights)\n",
    "    f = K.function(symb_inputs, grads)\n",
    "    x, y, sample_weight = model._standardize_user_data(inputs, outputs)\n",
    "    output_grad = f(x + y + sample_weight)\n",
    "    return output_grad\n",
    "\n",
    "weight_grads = get_weight_grad(model, train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QqtwBOlCq-ag"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wcNq8pN2q-al"
   },
   "outputs": [],
   "source": [
    "def max_weights(arr):\n",
    "    return abs(np.amax(arr))\n",
    "    \n",
    "max_grad = list(map(max_weights , weight_grads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wcNq8pN2q-al"
   },
   "outputs": [],
   "source": [
    "grad_df = pd.DataFrame(max_grad , columns=['absolute_max_grad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wcNq8pN2q-al"
   },
   "outputs": [],
   "source": [
    "grad_df['log'] = np.log(grad_df['absolute_max_grad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=grad_df.index , y = grad_df['absolute_max_grad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=grad_df.index , y = grad_df['log'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "going deeper.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
