{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks - recitation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```In this short excersice, you will experience with implementing neural network for binary classification task, and check it on a real dataset.\n",
    "It may give you a basic understanding about the model and it's limits, and above all, about how it works.```\n",
    "\n",
    "\n",
    "```So lets start by itroduction to some notions, that will help us use \"gradient descent\" methods to train networks.```\n",
    "\n",
    "```~Gild Royz```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain rule\n",
    "\n",
    "``for a given functions, h,f,g such that:``\n",
    "\n",
    "$$h(x) = f(g(x))$$\n",
    "\n",
    "``we know that:``\n",
    "\n",
    "$$\\frac{dh}{dx} = \\frac{df}{dg} \\cdot \\frac{dg}{dx}$$\n",
    "\n",
    "```For example: ```\n",
    "\n",
    "$f(x) = x^2$\n",
    "\n",
    "$g(x) = 2x + 5$\n",
    "\n",
    "$h(x) = f(g(x))$\n",
    "\n",
    "$$\\frac{dh}{dx} = 2(2x + 5) \\cdot 2 = 8x + 20$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions as computational graphs\n",
    "\n",
    "```Every function can be writen as a computational acyclic graph, where the nodes are the variables, and the egdes are the actions. Example:```\n",
    "\n",
    "$$f(x) = x^2 + 5x^3$$\n",
    "\n",
    "```Its computation graph is:```\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "```Ok cool. how is it helping us?\n",
    "Lets try to calculate ``` $\\frac{\\partial}{\\partial x}f$ ``` and see what we get:```\n",
    "\n",
    "($\\frac{\\partial}{\\partial x}f$ stands for deriving f according to x)\n",
    "\n",
    "$$\\frac{\\partial}{\\partial x}f = 2x + 15x^2$$\n",
    "\n",
    "``But because that f is composed of several functions, we can write it as follow:``\n",
    "\n",
    "$$f(x) = f(p(h(x), s(q(x))))$$\n",
    "\n",
    "``if we will derive the function according to the chan rule, we will get:``\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial p} \\cdot \\frac{\\partial p}{\\partial x} = \\frac{\\partial f}{\\partial p} \\cdot (\\frac{\\partial p}{\\partial h} \\cdot  \\frac{\\partial h}{\\partial x} + \\frac{\\partial p}{\\partial s} \\cdot  \\frac{\\partial s}{\\partial x}) = \\frac{\\partial f}{\\partial p} \\cdot (\\frac{\\partial p}{\\partial h} \\cdot  \\frac{\\partial h}{\\partial x} + \\frac{\\partial p}{\\partial s} \\cdot  \\frac{\\partial s}{\\partial q} \\cdot \\frac{\\partial q}{\\partial x}) = 1 \\cdot (1 \\cdot 2x + 1 \\cdot 5 \\cdot 3x^2) = 2x + 15x^2$$\n",
    "\n",
    "``We can conclude from here, that the derivation of a node u, according to earlier node v, is the sum of derivations of all paths between v -> u.``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, auc, roc_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```We will look at the \"breast cancer\" dataset, and try to build a neural network that fits it.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = datasets.load_breast_cancer()['data']\n",
    "Y = datasets.load_breast_cancer()['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Melody\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "train_X, test_X, train_Y, test_Y = train_test_split(X, Y, train_size =0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Lets Normalize the data to have \"mean 0\" and \"standard deviation 1\", in order to help the network deal with it.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "new_train_X = scaler.fit_transform(train_X)\n",
    "new_test_X = scaler.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras import Model\n",
    "# from keras.layers import Dense, Input\n",
    "# from keras.optimizers import SGD\n",
    "# from keras.losses import binary_crossentropy\n",
    "\n",
    "# inp = Input((30,))\n",
    "# #hidden = Dense(20, activation='relu')(inp)\n",
    "# hidden = Dense(10, activation='relu')(inp)\n",
    "# out = Dense(1, activation='sigmoid')(hidden)\n",
    "\n",
    "# model = Model(inp, out)\n",
    "\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# model.fit(new_train_X, train_Y, batch_size=10, epochs=70, validation_data=(new_test_X, test_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relu activation\n",
    "\n",
    "$$relu(x) = \\begin{cases} x, \\hspace{1cm} x \\geq 0 \\\\ 0, \\hspace{1cm} otherwize \\end{cases}$$\n",
    "\n",
    "$$\\frac{d}{dx} relu = \\begin{cases} 1, \\hspace{1cm} x \\geq 0 \\\\ 0, \\hspace{1cm} otherwize \\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    \n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "def relu_derive(x):\n",
    "    return np.maximum(x >= 0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid activation\n",
    "\n",
    "$$sigmoid(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "$$\\frac{d}{dx} sigmoid = sigmoid(x) \\cdot (1 - sigmoid(x))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_derive(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross entropy\n",
    "\n",
    "$y \\in {0,1}$ - true label,  $\\hspace{1cm}$  $\\hat{y} \\in [0,1]$ - predicted label\n",
    "\n",
    "$$cross-entropy(y,\\hat{y}) = - y * log(\\hat{y}) - (1 - y) * log(1 - \\hat{y})$$\n",
    "\n",
    "```When y is 0, we get punish if y_hat is bigger then 0.```\n",
    "\n",
    "```When when y is 1 we get punish if y_hat is smaller then 1.```\n",
    "\n",
    "and the derivative according to $\\hat{y}$ is:\n",
    "\n",
    "$$\\frac{d}{d \\hat{y}}cross-entropy(y,\\hat{y}) = - \\frac{y}{\\hat{y}} + \\frac{(1 - y)}{1 - \\hat{y}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy(y, y_hat, epsilon=1e-10):\n",
    "    \"\"\"\n",
    "    y:       true label.\n",
    "    y_hat:   predicted_label.\n",
    "    epsilon: small addition for not dividing by 0.\n",
    "    \"\"\"\n",
    "    \n",
    "    return - y * np.log(y_hat + epsilon) - (1 - y) * np.log(1 - y_hat + epsilon)\n",
    "\n",
    "def binary_cross_entropy_derive(y, y_hat, epsilon=1e-10):\n",
    "    \n",
    "    return - y / (y_hat + epsilon) + (1 - y) / (1 - y_hat + epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "```We will do a 2-layer fully connected NN with the following details:```\n",
    "\n",
    "- ```First layer  - 10 neurons with \"relu\" activation.```\n",
    "- ```Second layer - 1 neurons with \"sigmoid\" activation.```\n",
    "- ```Loss         - \"binary crossentropy\"```\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_full(X, w_1, u1, w_2, u2):\n",
    "    \"\"\"\n",
    "    X  : the data with shape (30 x N)\n",
    "    \n",
    "    w_1: first layer weights with shape (10 x 30)\n",
    "    u_1: first layer bias with shape (10 x 1)\n",
    "    \n",
    "    w_2: second layer weights with shape (1 x 10)\n",
    "    u_2: second layer bias with shape (1 x 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    a = np.matmul(w_1, X)\n",
    "    a_z = relu(a + u1)\n",
    "    \n",
    "    b = np.matmul(w_2, a_z)\n",
    "    b_z = sigmoid(b + u2)\n",
    "    \n",
    "    return a, a_z, b, b_z\n",
    "\n",
    "def predict(X, w_1, u1, w_2, u2):\n",
    "    \n",
    "    return predict_full(X, w_1, u1, w_2, u2)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_process(train_losses, train_accs, val_losses=None, val_accs=None):\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2)\n",
    "    fig.set_figwidth(10)\n",
    "    fig.set_figheight(5)\n",
    "    \n",
    "    ax[0].set_title('loss')\n",
    "    ax[0].plot(train_losses, label='train')\n",
    "    if val_losses is not None:\n",
    "        ax[0].plot(val_losses, label='val')\n",
    "    ax[0].set_xlabel('epoch')\n",
    "    ax[0].set_ylabel('loss')\n",
    "    ax[0].legend(loc=0)\n",
    "    \n",
    "    ax[1].set_title('accuracy')\n",
    "    ax[1].plot(train_accs, label='train')\n",
    "    if val_accs is not None:\n",
    "        ax[1].plot(val_accs, label='val')\n",
    "    ax[1].set_xlabel('epoch')\n",
    "    ax[1].set_ylabel('acc')\n",
    "    ax[1].legend(loc=4)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of the network\n",
    "\n",
    "```The network function is can be writen as follow:```\n",
    "\n",
    "``for a vector x with shape (30 x 1) (with corresponding y with value {0,1}) we have:``\n",
    "\n",
    "$$f(x) = sigmoid(w^{(2)} \\cdot relu(w^{(1)} \\cdot x + u^{(1)}) + u^{(2)})$$\n",
    "$$loss(x,y) = cross-entropy(y, f(x))$$\n",
    "\n",
    "``where:``\n",
    "- $w^{(1)}$ is a weight matrix with shape (10,30)\n",
    "- $u^{(1)}$ is a bias vector with shape (10,1)\n",
    "\n",
    "\n",
    "- $w^{(2)}$ is a weight matrix with shape (1,10)\n",
    "- $u^{(2)}$ is a bias vector with shape (1,1)\n",
    "\n",
    "``according to the chain rule, we can calculate the derivatives of:`` $w^{(1)}, u^{(1)}, w^{(2)}, u^{(2)}$\n",
    "\n",
    "``First lets make some markings that will make our life easer:``\n",
    "\n",
    "$a = w^{(1)} \\cdot x + u^{(1)}$ ``(shape: 10 x 1)``\n",
    "\n",
    "$a^{(z)} = relu(a)$ ``(output of layer 1) (shape: 10 x 1)``\n",
    "\n",
    "$b = w^{(2)} \\cdot a^{(z)} + u^{(2)}$ ``(shape: 1 x 1)``\n",
    "\n",
    "$b^{(z)} = sigmoid(b)$ ``(the network output) (shape: 1 x 1)``\n",
    "\n",
    "$loss(x,y) = cross-entropy(y, b^{(z)})$\n",
    "\n",
    "$$\\frac{\\partial loss}{\\partial w^{(2)}_{0,k}} = \\frac{\\partial loss}{\\partial b^{(z)}} \\cdot \\frac{\\partial b^{(z)}}{\\partial b} \\cdot \\frac{\\partial b}{\\partial w^{(2)}_{0,k}}$$\n",
    "\n",
    "$$\\frac{\\partial loss}{\\partial u^{(2)}_{0,0}} = \\frac{\\partial loss}{\\partial b^{(z)}} \\cdot \\frac{\\partial b^{(z)}}{\\partial b} \\cdot \\frac{\\partial b}{\\partial u^{(2)}_{0,0}}$$\n",
    "\n",
    "$$\\frac{\\partial loss}{\\partial w^{(1)}_{k,j}} = \\frac{\\partial loss}{\\partial b^{(z)}} \\cdot \\frac{\\partial b^{(z)}}{\\partial b} \\cdot \\frac{\\partial b}{\\partial a^{(z)}_k} \\cdot \\frac{\\partial a^{(z)}_k}{\\partial a_k} \\cdot \\frac{\\partial a_k}{\\partial w^{(1)}_{k,j}}$$\n",
    "\n",
    "$$\\frac{\\partial loss}{\\partial u^{(1)}_{k,0}} = \\frac{\\partial loss}{\\partial b^{(z)}} \\cdot \\frac{\\partial b^{(z)}}{\\partial b} \\cdot \\frac{\\partial b}{\\partial a^{(z)}_k} \\cdot \\frac{\\partial a^{(z)}_k}{\\partial a_k} \\cdot \\frac{\\partial a_k}{\\partial u^{(1)}_{k,0}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30, 54, 72, 31, 21, 95,  4, 60, 69, 86, 64, 73, 48, 66, 45, 87, 51,\n",
       "       80,  0,  6, 16, 15, 97, 40,  2, 99, 38, 56,  8, 32, 85, 13, 39, 35,\n",
       "       96, 77, 23, 24, 28, 65, 43, 74,  9, 62,  3, 10, 88, 61, 81, 67, 71,\n",
       "       33, 92, 49, 93, 36, 75, 44, 41, 83, 82, 18, 20, 78, 17,  7, 42, 70,\n",
       "       68, 94, 57, 79, 91,  1, 29, 89, 11, 34, 25, 58, 55, 37, 47, 19, 90,\n",
       "        5, 52, 53, 26, 63, 27, 50, 98, 76, 14, 59, 46, 12, 22, 84])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for me\n",
    "idx = np.random.choice(100, 100, False)\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 91)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<tokenize>\"\u001b[1;36m, line \u001b[1;32m91\u001b[0m\n\u001b[1;33m    Y_pred = predict(X, w_1, u_1, w_2, u_2)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "def fit_netwrok(X, Y, batch_size=10, epochs=20, lr=0.01, val=None):\n",
    "    \"\"\"\n",
    "    val: optional tuple with (validation_X, validation_Y).\n",
    "    \"\"\"\n",
    "    \n",
    "    N = len(X)\n",
    "    \n",
    "    X = X.T # transpose the data so it will fit the matrix multiplications\n",
    "    Y = Y.reshape(1,-1)\n",
    "    \n",
    "    w_1 = np.random.randn(10,30) ## choose random starting weights\n",
    "    u_1 = np.random.randn(10,1) ## choose random starting bias\n",
    "    \n",
    "    w_2 = np.random.randn(1,10) ## choose random starting weights\n",
    "    u_2 = np.random.randn(1,1) ## choose random starting bias\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses  = []\n",
    "    \n",
    "    train_accs = []\n",
    "    val_accs  = []\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        \n",
    "        idx = np.random.choice(N, N, False) ## choose random order of the x's, so the batchs will be different every epoch\n",
    "        \n",
    "    for j in range(0, N, batch_size):\n",
    "            \n",
    "            # take batch\n",
    "            \n",
    "            x = X[:,idx[j:j+batch_size]]\n",
    "            y = Y[:,idx[j:j+batch_size]]\n",
    "            n = y.shape[1]\n",
    "            \n",
    "            if n == 0:\n",
    "                continue\n",
    "            \n",
    "            a, a_z, b, b_z = predict_full(x, w_1, u_1, w_2, u_2) # preict with the model for all the batch\n",
    "\n",
    "            # define the gradients that we will fill\n",
    "            w_grad_1 = np.zeros(w_1.shape) \n",
    "            u_grad_1 = np.zeros(u_1.shape)\n",
    "\n",
    "            w_grad_2 = np.zeros(w_2.shape)\n",
    "            u_grad_2 = np.zeros(u_2.shape)\n",
    "            \n",
    "            ## we will calculate the gradient for every point in the batch, and then avarage\n",
    "            ## the graients of all the points in the batch. (Stochastic Gradient Descent)\n",
    "            \n",
    "            for i in range(n):\n",
    "            \n",
    "                xi, yi = x[:,i], y[:,i]\n",
    "            \n",
    "                #gradients: layer 2\n",
    "                \n",
    "                dL_dbz = # answer\n",
    "                dbz_db = # answer\n",
    "                db_dw2 = # answer\n",
    "                \n",
    "                for l in range(10):\n",
    "                    w_grad_2[0,l] += # answer\n",
    "                \n",
    "                u_grad_2[0] += # answer\n",
    "                \n",
    "                #gradients: layer 1\n",
    "                \n",
    "                db_daz = # answer\n",
    "                daz_da = # answer\n",
    "                da_dw1 = # answer\n",
    "                \n",
    "                for k in range(10):\n",
    "                    for l in range(30):\n",
    "                        w_grad_1[k,l] += # answer\n",
    "                \n",
    "                for k in range(10):\n",
    "                    u_grad_1[k] += # answer\n",
    "            \n",
    "            w_grad_2 /= n\n",
    "            u_grad_2 /= n\n",
    "            \n",
    "            w_grad_1 /= n\n",
    "            u_grad_1 /= n\n",
    "            \n",
    "            w_1 -= lr*w_grad_1\n",
    "            u_1 -= lr*u_grad_1\n",
    "\n",
    "            w_2 -= lr*w_grad_2\n",
    "            u_2 -= lr*u_grad_2\n",
    "\n",
    "\n",
    "        Y_pred = predict(X, w_1, u_1, w_2, u_2)\n",
    "\n",
    "        loss     = (1/N)*np.sum(binary_cross_entropy(Y, Y_pred))\n",
    "        accuracy = np.mean(Y_pred.round() == Y)\n",
    "        \n",
    "        train_losses += [loss]\n",
    "        train_accs += [accuracy]\n",
    "        \n",
    "        if val is not None:\n",
    "            \n",
    "            Y_pred = predict(val[0].T, w_1, u_1, w_2, u_2)\n",
    "            \n",
    "            loss     = (1/len(val[1]))*np.sum(binary_cross_entropy(val[1], Y_pred))\n",
    "            accuracy = np.mean(Y_pred.round() == val[1])\n",
    "\n",
    "            val_losses += [loss]\n",
    "            val_accs += [accuracy]\n",
    "    \n",
    "    plot_training_process(train_losses, train_accs, val_losses, val_accs)\n",
    "    \n",
    "    return w_1, u_1, w_2, u_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0004153434182532\n"
     ]
    }
   ],
   "source": [
    "params = fit_netwrok(new_train_X,train_Y, 10, 80, lr=0.01, val=(new_test_X, test_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict(new_test_X.T, *params).reshape(-1)\n",
    "auc_score = roc_auc_score(test_Y, y_pred)\n",
    "acc = np.mean(y_pred.round() == test_Y)\n",
    "matrix = confusion_matrix(test_Y, y_pred.round())\n",
    "print('test accuracy:\\t', acc)\n",
    "print('auc:\\t\\t', auc_score)\n",
    "print('confussion matrix:\\n', matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus\n",
    "\n",
    "```There is a way of calculating the gradients more efficiently (not calculating the same derivatives twice), and do it with matrix multiplications all the way.```\n",
    "\n",
    "```A nice explenation can be found here: ``` http://neuralnetworksanddeeplearning.com/chap2.html\n",
    "\n",
    "```Try to implement it yourself!```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_netwrok_efficient(X, Y, batch_size=10, epochs=20, lr=0.01, val=None):\n",
    "    \n",
    "    N = len(X)\n",
    "    \n",
    "    X = X.T\n",
    "    Y = Y.reshape(1,-1)\n",
    "    \n",
    "    w_1 = np.random.randn(10,30)\n",
    "    u_1 = np.random.randn(10,1)\n",
    "    \n",
    "    w_2 = np.random.randn(1,10)\n",
    "    u_2 = np.random.randn(1,1)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses  = []\n",
    "    \n",
    "    train_accs = []\n",
    "    val_accs  = []\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        \n",
    "        idx = np.random.choice(N, N, False)\n",
    "        \n",
    "        for j in range(0, N, batch_size):\n",
    "            \n",
    "            x = X[:,idx[j:j+batch_size]]\n",
    "            y = Y[:,idx[j:j+batch_size]]\n",
    "            n = y.shape[1]\n",
    "            \n",
    "            if n == 0:\n",
    "                continue\n",
    "        \n",
    "            a, a_z, b, b_z = predict_full(x, w_1, u_1, w_2, u_2)\n",
    "            \n",
    "            #gradients: layer 2\n",
    "            \n",
    "            delta_2 = # answer\n",
    "            w_grad_2 = # answer\n",
    "            u_grad_2 = # answer\n",
    "\n",
    "            #gradients: layer 1\n",
    "            \n",
    "            delta_1 = # answer\n",
    "            w_grad_1 = # answer\n",
    "            u_grad_1 = # answer\n",
    "\n",
    "            w_1 -= lr*w_grad_1\n",
    "            u_1 -= lr*u_grad_1\n",
    "\n",
    "            w_2 -= lr*w_grad_2\n",
    "            u_2 -= lr*u_grad_2\n",
    "\n",
    "\n",
    "        Y_pred = predict(X, w_1, u_1, w_2, u_2)\n",
    "\n",
    "        loss     = (1/N)*np.sum(binary_cross_entropy(Y, Y_pred))\n",
    "        accuracy = np.mean(Y_pred.round() == Y)\n",
    "        \n",
    "        train_losses += [loss]\n",
    "        train_accs += [accuracy]\n",
    "        \n",
    "        if val is not None:\n",
    "            \n",
    "            Y_pred = predict(val[0].T, w_1, u_1, w_2, u_2)\n",
    "            \n",
    "            loss     = (1/len(val[1]))*np.sum(binary_cross_entropy(val[1], Y_pred))\n",
    "            accuracy = np.mean(Y_pred.round() == val[1])\n",
    "\n",
    "            val_losses += [loss]\n",
    "            val_accs += [accuracy]\n",
    "    \n",
    "    plot_training_process(train_losses, train_accs, val_losses, val_accs)\n",
    "    \n",
    "    return w_1, u_1, w_2, u_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = fit_netwrok_efficient(new_train_X,train_Y, 10, 80, lr=0.01, val=(new_test_X, test_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict(new_test_X.T, *params).reshape(-1)\n",
    "auc_score = roc_auc_score(test_Y, y_pred)\n",
    "acc = np.mean(y_pred.round() == test_Y)\n",
    "matrix = confusion_matrix(test_Y, y_pred.round())\n",
    "print('test accuracy:\\t', acc)\n",
    "print('auc:\\t\\t', auc_score)\n",
    "print('confussion matrix:\\n', matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
