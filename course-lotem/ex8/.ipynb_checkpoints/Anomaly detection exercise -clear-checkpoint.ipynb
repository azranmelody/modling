{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Anomaly Detection\n",
    "```In this exercise you will use concepts you know, and maybe some concepts you are about to meet, in order to find anomalies in dataset of credit cards transactions.\n",
    "We will think about this problem as one think of real anomaly detecting problems: your goal will be to choose the 1,000 most anomalous samples from the dataset - the samples you suspect to be the anomaly samples. In real life problems, those samples will be handed to a human researcher for verification. Obviously, if you give him a lot of regular samples, he will get angry.```\n",
    "\n",
    "```~Ittai Haran```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Load the dataset. You can see it's labeled: It's for allowing you to test yourself. Note that in real life problems, you won't have it. Normalize the dataset as you see fit.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>...</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>...</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9       V10  ...         V20       V21       V22  \\\n",
       "0  0.098698  0.363787  0.090794  ...    0.251412 -0.018307  0.277838   \n",
       "1  0.085102 -0.255425 -0.166974  ...   -0.069083 -0.225775 -0.638672   \n",
       "2  0.247676 -1.514654  0.207643  ...    0.524980  0.247998  0.771679   \n",
       "3  0.377436 -1.387024 -0.054952  ...   -0.208038 -0.108300  0.005274   \n",
       "4 -0.270533  0.817739  0.753074  ...    0.408542 -0.009431  0.798278   \n",
       "\n",
       "        V23       V24       V25       V26       V27       V28  Class  \n",
       "0 -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053      0  \n",
       "1  0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724      0  \n",
       "2  0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752      0  \n",
       "3 -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458      0  \n",
       "4 -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153      0  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('D:/creditcard.csv')  ## can be found in: https://drive.google.com/open?id=1wyz2czVFaQWdqRmLAtT5MwCOSlnZ6od9\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Your first task is to formulate a method for evaluating your anomalies. Write an evaluation method, which will help you compare between different ways to detect anomalies. Notice that this isn't a classification method, and regard your true goal: to mark the 1,000 most anomalous samples.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_method(y_true, grades):\n",
    "    # y_true is the class: 0 for regular, 1 for anomaly\n",
    "    # the grades should indicate how anomalous you think the sample is - as higher the grade, the sample is more suspiciuos\n",
    "    # suggestion: sklearn.metrics.roc_auc_score(y_true, grades) -> read about it!\n",
    "    # but you are encouraged to create something of your own.\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    print('the ratio of anomaly among all data - >' , sum(y_true)/len(y_true))\n",
    "    most_anomalous_samples = pd.DataFrame(list(zip(y_true,grades))).sort_values(by=1, ascending=False)[:1000]\n",
    "    print('the ratio of anomaly among 1000 selected samples - >' , sum(most_anomalous_samples[0])/len(most_anomalous_samples))\n",
    "    print('roc auc score :',roc_auc_score(y_true,grades) )\n",
    "    return most_anomalous_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```We can now examine different methods for anomaly detecting. For each method, evaluate it, and compare it to the other methods.```\n",
    "\n",
    "```The first one we will try is to grade the samples by their distance from the 'mean sample', in units of standard deviation. You can also think about the features as independent gaussian distributions and grade a sample by its distance from the gaussian's mean, for every feature.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "std_each_feature = df.std()\n",
    "mean_each_feature = df.mean()\n",
    "\n",
    "df['grade_by_distance'] = df.apply(lambda x: abs(((x-mean_each_feature)/std_each_feature)).sum(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the ratio of anomaly among all data - > 0.001727485630620034\n",
      "the ratio of anomaly among 1000 selected samples - > 0.302\n",
      "roc auc score : 0.9957456296684557\n"
     ]
    }
   ],
   "source": [
    "most_anomaly_samples_try1 = evaluate_method(df['Class'],df['grade_by_distance'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```What hidden assumption you took during \"training\"? what part of the data you trained on?```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my assumption was that the \"good samples\" (not the anomaly) suppost to have some similarity bwtween each other\n",
    "# therefore the distnace between them suppost to be lower than the distance between them to the anomaly samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Try using PCA: project the dataset into a lower dimensional space, and than use the \"inverse\" transformation (why \"\"?) to get approximated samples. Compare the samples you got to the samples you started with.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "      <th>distnaces_from_boundary</th>\n",
       "      <th>grade_by_distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>...</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>0</td>\n",
       "      <td>1.063187</td>\n",
       "      <td>12.606685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>0</td>\n",
       "      <td>1.347840</td>\n",
       "      <td>10.464983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>...</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>0</td>\n",
       "      <td>0.818188</td>\n",
       "      <td>24.748699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>0</td>\n",
       "      <td>0.553057</td>\n",
       "      <td>18.061680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>...</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>0</td>\n",
       "      <td>0.940891</td>\n",
       "      <td>15.845231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9       V10        ...               V22       V23  \\\n",
       "0  0.098698  0.363787  0.090794        ...          0.277838 -0.110474   \n",
       "1  0.085102 -0.255425 -0.166974        ...         -0.638672  0.101288   \n",
       "2  0.247676 -1.514654  0.207643        ...          0.771679  0.909412   \n",
       "3  0.377436 -1.387024 -0.054952        ...          0.005274 -0.190321   \n",
       "4 -0.270533  0.817739  0.753074        ...          0.798278 -0.137458   \n",
       "\n",
       "        V24       V25       V26       V27       V28  Class  \\\n",
       "0  0.066928  0.128539 -0.189115  0.133558 -0.021053      0   \n",
       "1 -0.339846  0.167170  0.125895 -0.008983  0.014724      0   \n",
       "2 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752      0   \n",
       "3 -1.175575  0.647376 -0.221929  0.062723  0.061458      0   \n",
       "4  0.141267 -0.206010  0.502292  0.219422  0.215153      0   \n",
       "\n",
       "   distnaces_from_boundary  grade_by_distance  \n",
       "0                 1.063187          12.606685  \n",
       "1                 1.347840          10.464983  \n",
       "2                 0.818188          24.748699  \n",
       "3                 0.553057          18.061680  \n",
       "4                 0.940891          15.845231  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=5, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=5)\n",
    "pca.fit(df.drop(['Class','grade_by_distance'], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "components = pca.transform(df.drop(['Class','grade_by_distance'] , axis=1))\n",
    "projected = pca.inverse_transform(components)\n",
    "projected_df = pd.DataFrame(projected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distance_original_projected_df = abs(df.loc[:, 'V1':'V28'] - projected)\n",
    "distance_original_projected_df['grade_distance_orig_proj'] = distance_original_projected_df.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the ratio of anomaly among all data - > 0.001727485630620034\n",
      "the ratio of anomaly among 1000 selected samples - > 0.228\n",
      "roc auc score : 0.9548524059181468\n"
     ]
    }
   ],
   "source": [
    "most_anomaly_samples_try2 = evaluate_method(df['Class'],distance_original_projected_df['grade_distance_orig_proj'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['grade_distance_orig_proj'] = distance_original_projected_df['grade_distance_orig_proj']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Read about one class SVM. Use it to evaluate your samples. Notice that this algorithm is very slow compared to those you tried earlier. Consider training it only on a fraction of the samples.\n",
    "Hint: you can use the decision function directly to get the distance of the sample from the decision boundary.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "train = df.loc[:, 'V1':'V28']\n",
    "train = train[:int(len(train)*0.3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "oneclasssvm = OneClassSVM().fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distnaces_from_boundary = oneclasssvm.decision_function(df.loc[:, 'V1':'V28'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "df['distnaces_from_boundary'] = abs(scaler.fit_transform(distnaces_from_boundary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the ratio of anomaly among all data - > 0.001727485630620034\n",
      "the ratio of anomaly among 1000 selected samples - > 0.124\n",
      "roc auc score : 0.9427876214818987\n"
     ]
    }
   ],
   "source": [
    "most_anomaly_samples_try3 = evaluate_method(df['Class'] , df['distnaces_from_boundary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Now try clustering your data, and use the distance from the clusters (you will have to define it) to grade the samples. Think about changing your normalization method when trying to cluster. Here you also might want to consider to train on a fraction of the samples.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#clusters = oneclasssvm.predict(df.loc[:, 'V1':'V28'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Try combining the grades you got from different methods into a single grade. Did you get a better detector? why or why not?```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Melody\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  \n",
      "C:\\Users\\Melody\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\Melody\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "minmax = MinMaxScaler()\n",
    "df['distnaces_from_boundary_normalized'] = minmax.fit_transform(df['distnaces_from_boundary'].reshape(-1,1))\n",
    "df['grade_by_distance_normalized'] = minmax.fit_transform(df['grade_by_distance'].reshape(-1,1))\n",
    "df['grade_distance_orig_proj_normalized'] = minmax.fit_transform(df['grade_distance_orig_proj'].reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "      <th>distnaces_from_boundary</th>\n",
       "      <th>grade_by_distance</th>\n",
       "      <th>grade_distance_orig_proj</th>\n",
       "      <th>mean_grades</th>\n",
       "      <th>distnaces_from_boundary_normalized</th>\n",
       "      <th>grade_by_distance_normalized</th>\n",
       "      <th>grade_distance_orig_proj_normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>...</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>0</td>\n",
       "      <td>1.063187</td>\n",
       "      <td>12.606685</td>\n",
       "      <td>7.561701</td>\n",
       "      <td>12.606685</td>\n",
       "      <td>0.374363</td>\n",
       "      <td>0.005470</td>\n",
       "      <td>0.006934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>0</td>\n",
       "      <td>1.347840</td>\n",
       "      <td>10.464983</td>\n",
       "      <td>7.237555</td>\n",
       "      <td>10.464983</td>\n",
       "      <td>0.474594</td>\n",
       "      <td>0.002335</td>\n",
       "      <td>0.006210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>0</td>\n",
       "      <td>0.818188</td>\n",
       "      <td>24.748699</td>\n",
       "      <td>18.599587</td>\n",
       "      <td>24.748699</td>\n",
       "      <td>0.288094</td>\n",
       "      <td>0.023245</td>\n",
       "      <td>0.031595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>0</td>\n",
       "      <td>0.553057</td>\n",
       "      <td>18.061680</td>\n",
       "      <td>12.806556</td>\n",
       "      <td>18.061680</td>\n",
       "      <td>0.194737</td>\n",
       "      <td>0.013456</td>\n",
       "      <td>0.018652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>...</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>0</td>\n",
       "      <td>0.940891</td>\n",
       "      <td>15.845231</td>\n",
       "      <td>10.721230</td>\n",
       "      <td>15.845231</td>\n",
       "      <td>0.331300</td>\n",
       "      <td>0.010211</td>\n",
       "      <td>0.013993</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9       V10                 ...                   \\\n",
       "0  0.098698  0.363787  0.090794                 ...                    \n",
       "1  0.085102 -0.255425 -0.166974                 ...                    \n",
       "2  0.247676 -1.514654  0.207643                 ...                    \n",
       "3  0.377436 -1.387024 -0.054952                 ...                    \n",
       "4 -0.270533  0.817739  0.753074                 ...                    \n",
       "\n",
       "        V27       V28  Class  distnaces_from_boundary  grade_by_distance  \\\n",
       "0  0.133558 -0.021053      0                 1.063187          12.606685   \n",
       "1 -0.008983  0.014724      0                 1.347840          10.464983   \n",
       "2 -0.055353 -0.059752      0                 0.818188          24.748699   \n",
       "3  0.062723  0.061458      0                 0.553057          18.061680   \n",
       "4  0.219422  0.215153      0                 0.940891          15.845231   \n",
       "\n",
       "   grade_distance_orig_proj  mean_grades  distnaces_from_boundary_normalized  \\\n",
       "0                  7.561701    12.606685                            0.374363   \n",
       "1                  7.237555    10.464983                            0.474594   \n",
       "2                 18.599587    24.748699                            0.288094   \n",
       "3                 12.806556    18.061680                            0.194737   \n",
       "4                 10.721230    15.845231                            0.331300   \n",
       "\n",
       "   grade_by_distance_normalized  grade_distance_orig_proj_normalized  \n",
       "0                      0.005470                             0.006934  \n",
       "1                      0.002335                             0.006210  \n",
       "2                      0.023245                             0.031595  \n",
       "3                      0.013456                             0.018652  \n",
       "4                      0.010211                             0.013993  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['mean_grades'] = df[['distnaces_from_boundary_normalized','grade_by_distance_normalized', 'grade_distance_orig_proj_normalized']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the ratio of anomaly among all data - > 0.001727485630620034\n",
      "the ratio of anomaly among 1000 selected samples - > 0.266\n",
      "roc auc score : 0.9523939867452067\n"
     ]
    }
   ],
   "source": [
    "combine_grades_anomaly = evaluate_method(df['Class'],df['mean_grades'])\n",
    "# i tried to combine the grade with\\without normalize them, tried mean/sum/max of those three\n",
    "# grades and couldn't get a better grade than the first one we did. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Now we will experience with Deep Auto Encoders. The idea is to create a neural network that gets the samples as input, and try to predict the very same samples: The difficulty comes from the fact that the networks gets narrower, and so having an information bottleneck. The grade each sample will get is the reconstruction error - the difference between the output and the input. You can read more about Auto Encoders in the literature.\n",
    "(If you want to know more about Auto Encoders, read also about about Variational Auto Encoder)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras import regularizers\n",
    "\n",
    "train_X = df.loc[:, 'V1':'V28']\n",
    "input_dim = train_X.shape[1] # the # features\n",
    "encoding_dim = 16 # first layer\n",
    "hidden_dim = int(encoding_dim / 2) #hideen layer\n",
    "\n",
    "nb_epoch = 30\n",
    "batch_size = 128\n",
    "learning_rate = 0.1\n",
    "\n",
    "input_layer = Input(shape=(input_dim, ))\n",
    "encoder = Dense(encoding_dim, activation=\"tanh\")(input_layer)\n",
    "encoder = Dense(hidden_dim, activation=\"relu\")(encoder)\n",
    "decoder = Dense(encoding_dim, activation='relu')(encoder)\n",
    "decoder = Dense(input_dim, activation='tanh')(decoder)\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "autoencoder.compile(optimizer='adam',loss='mse', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "284807/284807 [==============================] - 6s 20us/step - loss: 0.7421 - accuracy: 0.4670\n",
      "Epoch 2/30\n",
      "284807/284807 [==============================] - 6s 20us/step - loss: 0.6438 - accuracy: 0.4992\n",
      "Epoch 3/30\n",
      "284807/284807 [==============================] - 5s 19us/step - loss: 0.6281 - accuracy: 0.5024\n",
      "Epoch 4/30\n",
      "284807/284807 [==============================] - 5s 18us/step - loss: 0.6160 - accuracy: 0.4933\n",
      "Epoch 5/30\n",
      "284807/284807 [==============================] - 5s 18us/step - loss: 0.6077 - accuracy: 0.4837\n",
      "Epoch 6/30\n",
      "284807/284807 [==============================] - 5s 18us/step - loss: 0.6024 - accuracy: 0.4802\n",
      "Epoch 7/30\n",
      "284807/284807 [==============================] - 5s 18us/step - loss: 0.5990 - accuracy: 0.4721\n",
      "Epoch 8/30\n",
      "284807/284807 [==============================] - 6s 20us/step - loss: 0.5967 - accuracy: 0.4624\n",
      "Epoch 9/30\n",
      "284807/284807 [==============================] - 5s 18us/step - loss: 0.5950 - accuracy: 0.4537\n",
      "Epoch 10/30\n",
      "284807/284807 [==============================] - 5s 18us/step - loss: 0.5935 - accuracy: 0.4460\n",
      "Epoch 11/30\n",
      "284807/284807 [==============================] - 5s 17us/step - loss: 0.5923 - accuracy: 0.4405\n",
      "Epoch 12/30\n",
      "284807/284807 [==============================] - 5s 17us/step - loss: 0.5911 - accuracy: 0.4363\n",
      "Epoch 13/30\n",
      "284807/284807 [==============================] - 5s 17us/step - loss: 0.5901 - accuracy: 0.4341\n",
      "Epoch 14/30\n",
      "284807/284807 [==============================] - 5s 17us/step - loss: 0.5892 - accuracy: 0.4337\n",
      "Epoch 15/30\n",
      "284807/284807 [==============================] - 5s 17us/step - loss: 0.5885 - accuracy: 0.4336\n",
      "Epoch 16/30\n",
      "284807/284807 [==============================] - 5s 17us/step - loss: 0.5877 - accuracy: 0.4348\n",
      "Epoch 17/30\n",
      "284807/284807 [==============================] - 5s 17us/step - loss: 0.5874 - accuracy: 0.4363\n",
      "Epoch 18/30\n",
      "284807/284807 [==============================] - 5s 17us/step - loss: 0.5866 - accuracy: 0.4381\n",
      "Epoch 19/30\n",
      "284807/284807 [==============================] - 5s 17us/step - loss: 0.5862 - accuracy: 0.4389\n",
      "Epoch 20/30\n",
      "284807/284807 [==============================] - 5s 17us/step - loss: 0.5859 - accuracy: 0.4410\n",
      "Epoch 21/30\n",
      "284807/284807 [==============================] - 5s 17us/step - loss: 0.5857 - accuracy: 0.4419\n",
      "Epoch 22/30\n",
      "284807/284807 [==============================] - 5s 17us/step - loss: 0.5852 - accuracy: 0.4418\n",
      "Epoch 23/30\n",
      "284807/284807 [==============================] - 5s 17us/step - loss: 0.5850 - accuracy: 0.4431\n",
      "Epoch 24/30\n",
      "284807/284807 [==============================] - 5s 17us/step - loss: 0.5847 - accuracy: 0.4420\n",
      "Epoch 25/30\n",
      "284807/284807 [==============================] - 5s 17us/step - loss: 0.5845 - accuracy: 0.4416\n",
      "Epoch 26/30\n",
      "284807/284807 [==============================] - 5s 17us/step - loss: 0.5843 - accuracy: 0.4411\n",
      "Epoch 27/30\n",
      "284807/284807 [==============================] - 5s 17us/step - loss: 0.5840 - accuracy: 0.4408\n",
      "Epoch 28/30\n",
      "284807/284807 [==============================] - 5s 17us/step - loss: 0.5838 - accuracy: 0.4401\n",
      "Epoch 29/30\n",
      "284807/284807 [==============================] - 5s 17us/step - loss: 0.5835 - accuracy: 0.4405\n",
      "Epoch 30/30\n",
      "284807/284807 [==============================] - 5s 17us/step - loss: 0.5838 - accuracy: 0.4399\n"
     ]
    }
   ],
   "source": [
    "history = autoencoder.fit(train_X, train_X,\n",
    "                    epochs=nb_epoch,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    verbose=1).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = autoencoder.predict(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mse_list =[]\n",
    "for i in range(len(train_X)):\n",
    "    mse_list.append(mean_squared_error(train_X.iloc[i],predictions[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['grades_auto_encoder'] = mse_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the ratio of anomaly among all data - > 0.001727485630620034\n",
      "the ratio of anomaly among 1000 selected samples - > 0.169\n",
      "roc auc score : 0.9531876930274148\n"
     ]
    }
   ],
   "source": [
    "most_anomaly_samples_nn = evaluate_method(df['Class'],df['grades_auto_encoder'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Try thinking about other methods to detect anomalies in your data, and find a way to get better results.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1c170c33f60>"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c170c0fba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set(rc={'figure.figsize':(15,8)})\n",
    "sns.boxplot(x=\"variable\", y=\"value\", data=pd.melt(train_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_anomlay(row):\n",
    "    if (row < -30).any():\n",
    "        return 1\n",
    "    if (row >25).any():\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find anomaly according to the graph\n",
    "df['ind_anomaly'] = df.apply(lambda x: find_anomlay(x[:28]) , axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    83\n",
       "1    17\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['ind_anomaly']==1]['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['ind_anomaly']==1]['Class'].value_counts()[1] / len(df[df['ind_anomaly']==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
